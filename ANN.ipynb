{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.datasets import load_digits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "digits = load_digits()\n",
    "data = digits.data\n",
    "image = data[9].reshape(8,8)\n",
    "plt.imshow(image)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Data:\n",
    "    def __init__(self,x, y, batch_size):\n",
    "        self.x = x\n",
    "        self.y = y\n",
    "        self.l = x.shape[1]\n",
    "        self.batch_size = batch_size\n",
    "        self.pos = 0\n",
    "        \n",
    "    def forward(self):\n",
    "        #Mini-batch\n",
    "        pos = self.pos\n",
    "        bat = self.batch_size\n",
    "        l = self.l\n",
    "        if pos + bat >= l:\n",
    "            ret = (self.x[:,pos:l], self.y[pos:l])\n",
    "            self.pos = 0\n",
    "            index = range(l)\n",
    "            np.random.shuffle(list(index))\n",
    "            self.x = self.x[:,index]\n",
    "            self.y = self.y[index]\n",
    "        else:\n",
    "            ret = (self.x[:,pos:pos+bat], self.y[pos:pos+bat])\n",
    "            self.pos += self.batch_size\n",
    "        \n",
    "        return ret, self.pos\n",
    "    def backward(self, d):\n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 328,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FullyConnect:\n",
    "    def __init__(self, l_x, l_y, L2 = 0, keep_prob = 1, methods = 'Grad',\n",
    "                 k1=0.9, k2=0.999, batch_normal = False, predict = False):\n",
    "        np.random.seed(42)\n",
    "        self.l_x = l_x\n",
    "        self.weights = np.random.randn(l_y, l_x)*np.sqrt(2/l_x)\n",
    "        self.bias = np.random.randn(l_y, 1)\n",
    "        self.lr = 0\n",
    "        self.L2 = L2\n",
    "        self.keep_prob = keep_prob\n",
    "        self.methods = methods\n",
    "        #Monmentum\n",
    "        self.vdw = 0\n",
    "        self.vdb = 0\n",
    "        #RmSprop\n",
    "        self.sdw = 0\n",
    "        self.sdb = 0\n",
    "        #Adam\n",
    "        \n",
    "        #batch-Normalization\n",
    "        self.batch_normal =  batch_normal\n",
    "        self.gram = np.random.randn(l_y, 1)\n",
    "        self.beta = np.random.randn(l_y, 1)\n",
    "        self.mean = np.zeros((l_y, 1))\n",
    "        self.std = np.zeros((l_y, 1))\n",
    "        self.predict = predict\n",
    "        \n",
    "    def forward(self, x):\n",
    "        #drop-out\n",
    "        iskeep = np.random.rand(1, self.l_x)<self.keep_prob\n",
    "        self.keep_weights = self.weights*iskeep/self.keep_prob\n",
    "        \n",
    "        self.x = x\n",
    "        \n",
    "        self.y = np.dot(self.keep_weights, self.x)+self.bias\n",
    "        \n",
    "        # batch-Normalization\n",
    "        if self.batch_normal:\n",
    "            self._batch_normalization()\n",
    "        \n",
    "        return self.y \n",
    "    \n",
    "    def _batch_normalization(self):\n",
    "        if self.predict:\n",
    "            predict_y_norm = (self.y-self.mean)/(self.std+1e-8)\n",
    "            self.y = self.gram*predict_y_norm + self.beta\n",
    "        else:\n",
    "            n = self.y.shape[1]\n",
    "            self.tmean = np.mean(self.y, axis = 1, keepdims=True)\n",
    "            self.tstd = np.std(self.y, axis = 1, keepdims=True)\n",
    "            self.y_norm = (self.y-self.tmean)/(self.tstd+1e-8)\n",
    "            self.y = self.gram*self.y_norm + self.beta\n",
    "\n",
    "            self.mean = 0.9*self.mean + 0.1*self.tmean\n",
    "            self.std = 0.9*self.std + 0.1*self.tstd\n",
    "    \n",
    "    def backward(self, d):\n",
    "        if self.batch_normal:\n",
    "            d = d*self.gram/self.tstd\n",
    "            self.dgram = d*self.y_norm\n",
    "            self.gram -= self.lr*np.sum(self.dgram, axis = 1, keepdims=True)/self.y.shape[1]\n",
    "            self.beta -= self.lr*np.sum(d, axis = 1, keepdims=True)/self.y.shape[1]\n",
    "            \n",
    "        self.dw = np.dot(d, self.x.T)/self.x.shape[1]+self.L2*self.keep_weights/(2*self.x.shape[1])\n",
    "        self.db = np.sum(d, axis = 1, keepdims=True)/self.x.shape[1]\n",
    "        self.dx = np.dot(self.keep_weights.T, d)\n",
    "        \n",
    "        #优化\n",
    "        self._optimize(self.methods)\n",
    "        return self.dx\n",
    "    def _optimize(self, methods='Grad', k1=0.9, k2=0.999):\n",
    "        if methods == 'Grad':\n",
    "            self.weights -= self.lr * self.dw\n",
    "            self.bias -= self.lr * self.db\n",
    "        elif methods == 'Monmentum':\n",
    "            #未修正\n",
    "            self.vdw = k1*self.vdw+(1-k1)*self.dw\n",
    "            self.vdb = k1*self.vdb+(1-k1)*self.db\n",
    "            self.weights -= self.lr * self.vdw\n",
    "            self.bias -= self.lr * self.vdb\n",
    "        elif methods == 'RMSprop':\n",
    "            #未修正\n",
    "            self.sdw = k2*self.sdw + (1-k2)*self.dw**2\n",
    "            self.sdb = k2*self.sdb + (1-k2)*self.db**2\n",
    "            self.weights -= self.lr*self.dw/(np.sqrt(self.sdw)+1e-8)\n",
    "            self.bias -= self.lr*self.db/(np.sqrt(self.sdb)+1e-8)\n",
    "        elif methods == 'Adam':\n",
    "            self.vdw = k1*self.vdw+(1-k1)*self.dw\n",
    "            self.vdb = k1*self.vdb+(1-k1)*self.db\n",
    "            self.sdw = k2*self.sdw + (1-k2)*self.dw**2\n",
    "            self.sdb = k2*self.sdb + (1-k2)*self.db**2\n",
    "            \n",
    "            self.weights -= self.lr*self.vdw/(np.sqrt(self.sdw)+1e-8)\n",
    "            self.bias -= self.lr*self.vdb/(np.sqrt(self.sdb)+1e-8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 329,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Sigmoid:\n",
    "    def __init__(self):\n",
    "        pass\n",
    "    def sigmoid(self, x):\n",
    "        return 1/(1+np.exp(-x))\n",
    "    def forward(self, x):\n",
    "        self.x = x\n",
    "        self.y = self.sigmoid(x)\n",
    "        return self.y\n",
    "    def backward(self, d):\n",
    "        sig = self.sigmoid(self.x)\n",
    "        self.dx = d*sig*(1-sig)\n",
    "        return self.dx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 330,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Relu1:\n",
    "    def __init__(self):\n",
    "        pass\n",
    "    def relu1(self, x):\n",
    "        s = np.ones_like(x)/10\n",
    "        s[x > 0] = 1\n",
    "        return x*s\n",
    "    def forward(self, x):\n",
    "        self.x = x\n",
    "        self.y = self.relu1(x)\n",
    "        return self.y\n",
    "    def backward(self, d):\n",
    "        s = np.ones_like(self.x)/10\n",
    "        s[self.x > 0] = 1\n",
    "        return d*s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 331,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Relu:\n",
    "    def __init__(self):\n",
    "        pass\n",
    "    def relu(self, x):\n",
    "        return x*(x>0)\n",
    "    def forward(self, x):\n",
    "        self.x = x\n",
    "        self.y = self.relu(x)\n",
    "        return self.y\n",
    "    def backward(self, d):\n",
    "        r = self.x > 0\n",
    "        return d*r"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 332,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Tanh:\n",
    "    def __init__(self):\n",
    "        pass\n",
    "    def tanh(self, x):\n",
    "        return (np.exp(x)-np.exp(-x))/(np.exp(x)+np.exp(-x))\n",
    "    def forward(self, x):\n",
    "        self.x = x\n",
    "        self.y = self.tanh(x)\n",
    "        return self.y\n",
    "    def backward(self, d):\n",
    "        t = 1-self.tanh(self.x)**2\n",
    "        return d*t"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 333,
   "metadata": {},
   "outputs": [],
   "source": [
    "class QuadraticLoss:\n",
    "    def __init__(self, L2):\n",
    "        self.L2 = L2\n",
    "    def forward(self, x, label):\n",
    "        self.x = x\n",
    "        self.label = np.zeros_like(x)\n",
    "        for i in range(len(label)):\n",
    "            self.label[label[i], i] = 1\n",
    "        self.loss = np.sum(np.square(self.x - self.label))/self.x.shape[1]/2\n",
    "        return self.loss\n",
    "    def backward(self):\n",
    "        self.dx = (self.x - self.label)\n",
    "        return self.dx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 334,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Accuracy:\n",
    "    def __init__(self):\n",
    "        pass\n",
    "    def forward(self, x, label):\n",
    "        self.accuracy = 0\n",
    "        for i in range(len(label)):\n",
    "            xx = np.argmax(x[:, i])\n",
    "            if xx == label[i]:\n",
    "                self.accuracy += 1\n",
    "        self.accuracy = 1.0*self.accuracy/x.shape[1]\n",
    "        return self.accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 335,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ANN:\n",
    "    def __init__(self, layer_sizes, epochs = 20,batch_size = 1, learning_rate = 0.01, L2 = 0, keep_probs=None,\n",
    "                methods = 'Grad', k1=0.9, k2=0.999, batch_normal = False):\n",
    "        self.ls = layer_sizes\n",
    "        self.bs = batch_size\n",
    "        self.lr = learning_rate\n",
    "        self.epochs = epochs\n",
    "        \n",
    "        #正则化\n",
    "        self.L2 = L2\n",
    "        self.keeｐ_probs = keep_probs\n",
    "        \n",
    "        #优化算法\n",
    "        self.methods = methods\n",
    "        self.k1 = k1\n",
    "        self.k2 = k2\n",
    "        \n",
    "        #batch-normal\n",
    "        self.batch_normal = batch_normal\n",
    "        \n",
    "    def fit(self, X, y):\n",
    "        data_layer = Data(X, y, self.bs)\n",
    "        input_size = X.shape[0]\n",
    "        out_size = len(set(y))\n",
    "        inner_layers = []   \n",
    "        \n",
    "        losslayer = QuadraticLoss(0)\n",
    "        \n",
    "        if self.ls == []:\n",
    "            inner_layers.append(FullyConnect(input_size, out_size, self.L2))\n",
    "            inner_layers.append(Sigmoid())\n",
    "        elif self.ls != None:\n",
    "            inner_layers.append(FullyConnect(input_size, self.ls[0], self.L2, batch_normal=self.batch_normal))\n",
    "            inner_layers.append(Relu1())\n",
    "            \n",
    "            for i in range(0, len(self.ls)-1):\n",
    "                full_layer = FullyConnect(self.ls[i], self.ls[i+1], self.L2, batch_normal=self.batch_normal)\n",
    "                if self.keep_probs != None:\n",
    "                    full_layer.keep_prob = self.keep_probs[i]\n",
    "                inner_layers.append(full_layer)\n",
    "                inner_layers.append(Relu1())\n",
    "            \n",
    "            inner_layers.append(FullyConnect(self.ls[-1], out_size, self.L2))\n",
    "            inner_layers.append(Sigmoid())\n",
    "            \n",
    "        for layer in inner_layers:\n",
    "            layer.lr = self.lr # 为所有中间层设置学习速率\n",
    "            \n",
    "            layer.methods = self.methods\n",
    "            layer.k1 = self.k1\n",
    "            layer.k2 = self.k2\n",
    "        \n",
    "        print(len(inner_layers))\n",
    "        #学习\n",
    "        for i in range(self.epochs):\n",
    "            losssum = 0\n",
    "            iters = 0\n",
    "            print('epochs:', i)   \n",
    "            while True:\n",
    "                data, pos = data_layer.forward()  # 从数据层取出数据\n",
    "                x, label = data\n",
    "                for layer in inner_layers:  # 前向计算\n",
    "                    x = layer.forward(x)\n",
    "                \n",
    "                loss = losslayer.forward(x, label)  # 调用损失层forward函数计算损失函数值\n",
    "                losssum += loss\n",
    "                iters += 1\n",
    "                d = losslayer.backward()  # 调用损失层backward函数层计算将要反向传播的梯度\n",
    "                for layer in inner_layers[::-1]:  # 反向传播\n",
    "                    d = layer.backward(d)\n",
    "                if pos == 0:\n",
    "                    print('loss:', losssum / iters)\n",
    "                    break\n",
    "        self.inner_layers = inner_layers\n",
    "    \n",
    "    def predict(self,X):\n",
    "        for layer in self.inner_layers:\n",
    "            layer.predict = True\n",
    "            X = layer.forward(X)\n",
    "        return Ｘ\n",
    "    \n",
    "    def accuracy(self, y2, y):\n",
    "        accuracy = Accuracy()\n",
    "        accu = accuracy.forward(y2, y)  # 调用准确率层forward()函数求出准确率\n",
    "        print('accuracy:', accu)\n",
    "        return accu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 336,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "digits = load_digits()\n",
    "data = digits.data\n",
    "target = digits.target\n",
    "scaler = StandardScaler()\n",
    "data = scaler.fit_transform(data)\n",
    "\n",
    "train_x = data[:1500].T; train_y = target[:1500]\n",
    "test_x = data[1500:].T; test_y = target[1500:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 358,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6\n",
      "epochs: 0\n",
      "loss: 0.8051315179038282\n",
      "epochs: 1\n",
      "loss: 0.4178878706313635\n",
      "epochs: 2\n",
      "loss: 0.3791494718538582\n",
      "epochs: 3\n",
      "loss: 0.3552551789991108\n",
      "epochs: 4\n",
      "loss: 0.32692738108086605\n",
      "epochs: 5\n",
      "loss: 0.2998295673475612\n",
      "epochs: 6\n",
      "loss: 0.267853201004654\n",
      "epochs: 7\n",
      "loss: 0.2389520097548169\n",
      "epochs: 8\n",
      "loss: 0.21531962854908393\n",
      "epochs: 9\n",
      "loss: 0.19667778876925934\n",
      "epochs: 10\n",
      "loss: 0.1819457304945853\n",
      "epochs: 11\n",
      "loss: 0.16912839818818745\n",
      "epochs: 12\n",
      "loss: 0.1573247003241723\n",
      "epochs: 13\n",
      "loss: 0.1466291743930462\n",
      "epochs: 14\n",
      "loss: 0.1374405129697011\n",
      "epochs: 15\n",
      "loss: 0.12937390077776745\n",
      "epochs: 16\n",
      "loss: 0.12257054658727641\n",
      "epochs: 17\n",
      "loss: 0.11684805591467934\n",
      "epochs: 18\n",
      "loss: 0.1119067762973928\n",
      "epochs: 19\n",
      "loss: 0.10753585413837834\n",
      "epochs: 20\n",
      "loss: 0.10348976795583807\n",
      "epochs: 21\n",
      "loss: 0.09896073163789589\n",
      "epochs: 22\n",
      "loss: 0.0908177465610815\n",
      "epochs: 23\n",
      "loss: 0.08689340037303829\n",
      "epochs: 24\n",
      "loss: 0.08099409883045415\n",
      "epochs: 25\n",
      "loss: 0.07553182683678948\n",
      "epochs: 26\n",
      "loss: 0.07078733635520797\n",
      "epochs: 27\n",
      "loss: 0.06579496260879865\n",
      "epochs: 28\n",
      "loss: 0.06090947536377178\n",
      "epochs: 29\n",
      "loss: 0.057068145465400566\n",
      "epochs: 30\n",
      "loss: 0.053931557476551546\n",
      "epochs: 31\n",
      "loss: 0.051011351843780174\n",
      "epochs: 32\n",
      "loss: 0.04834938278479565\n",
      "epochs: 33\n",
      "loss: 0.04577148488038729\n",
      "epochs: 34\n",
      "loss: 0.043397938808314905\n",
      "epochs: 35\n",
      "loss: 0.04122959042668844\n",
      "epochs: 36\n",
      "loss: 0.039238683891568614\n",
      "epochs: 37\n",
      "loss: 0.0374953315650546\n",
      "epochs: 38\n",
      "loss: 0.035893530587940294\n",
      "epochs: 39\n",
      "loss: 0.034412590351235055\n",
      "epochs: 40\n",
      "loss: 0.032950548689965324\n",
      "epochs: 41\n",
      "loss: 0.031662541500601694\n",
      "epochs: 42\n",
      "loss: 0.03040010007073747\n",
      "epochs: 43\n",
      "loss: 0.02921252334025407\n",
      "epochs: 44\n",
      "loss: 0.027949086367111065\n",
      "epochs: 45\n",
      "loss: 0.02684211558648073\n",
      "epochs: 46\n",
      "loss: 0.02570497039333004\n",
      "epochs: 47\n",
      "loss: 0.024741046742953566\n",
      "epochs: 48\n",
      "loss: 0.023822781248540762\n",
      "epochs: 49\n",
      "loss: 0.022963827535134807\n",
      "epochs: 50\n",
      "loss: 0.02210809473251196\n",
      "epochs: 51\n",
      "loss: 0.021283056089203663\n",
      "epochs: 52\n",
      "loss: 0.020474732935772552\n",
      "epochs: 53\n",
      "loss: 0.019621520674799175\n",
      "epochs: 54\n",
      "loss: 0.018891825874239012\n",
      "epochs: 55\n",
      "loss: 0.018227655390290882\n",
      "epochs: 56\n",
      "loss: 0.017610602260526572\n",
      "epochs: 57\n",
      "loss: 0.017017937894956363\n",
      "epochs: 58\n",
      "loss: 0.016509972350211784\n",
      "epochs: 59\n",
      "loss: 0.016046835247963773\n",
      "epochs: 60\n",
      "loss: 0.015567479138515796\n",
      "epochs: 61\n",
      "loss: 0.0151303620291672\n",
      "epochs: 62\n",
      "loss: 0.014689337660382771\n",
      "epochs: 63\n",
      "loss: 0.0142507573235425\n",
      "epochs: 64\n",
      "loss: 0.013782834667120986\n",
      "epochs: 65\n",
      "loss: 0.0133134414880433\n",
      "epochs: 66\n",
      "loss: 0.012852876112732677\n",
      "epochs: 67\n",
      "loss: 0.012440269129898263\n",
      "epochs: 68\n",
      "loss: 0.012039409895963341\n",
      "epochs: 69\n",
      "loss: 0.011678716827051273\n",
      "epochs: 70\n",
      "loss: 0.011325170331057677\n",
      "epochs: 71\n",
      "loss: 0.010982646029959424\n",
      "epochs: 72\n",
      "loss: 0.010671457485551712\n",
      "epochs: 73\n",
      "loss: 0.010352480879802494\n",
      "epochs: 74\n",
      "loss: 0.010073355902954639\n",
      "epochs: 75\n",
      "loss: 0.009805619659613594\n",
      "epochs: 76\n",
      "loss: 0.00956441840689435\n",
      "epochs: 77\n",
      "loss: 0.009324674653687472\n",
      "epochs: 78\n",
      "loss: 0.009105426708094852\n",
      "epochs: 79\n",
      "loss: 0.008886222572270884\n",
      "epochs: 80\n",
      "loss: 0.008664300254745258\n",
      "epochs: 81\n",
      "loss: 0.008464311724487639\n",
      "epochs: 82\n",
      "loss: 0.008269888889054037\n",
      "epochs: 83\n",
      "loss: 0.008073965869653369\n",
      "epochs: 84\n",
      "loss: 0.007879114417314191\n",
      "epochs: 85\n",
      "loss: 0.007697361980250902\n",
      "epochs: 86\n",
      "loss: 0.00750086888220905\n",
      "epochs: 87\n",
      "loss: 0.007330265940218979\n",
      "epochs: 88\n",
      "loss: 0.007161537954217615\n",
      "epochs: 89\n",
      "loss: 0.007008571114647074\n",
      "epochs: 90\n",
      "loss: 0.00686977066988896\n",
      "epochs: 91\n",
      "loss: 0.006728790474411955\n",
      "epochs: 92\n",
      "loss: 0.006615301482052297\n",
      "epochs: 93\n",
      "loss: 0.006478136838910308\n",
      "epochs: 94\n",
      "loss: 0.006356101681455346\n",
      "epochs: 95\n",
      "loss: 0.006239762353177924\n",
      "epochs: 96\n",
      "loss: 0.006140028065877621\n",
      "epochs: 97\n",
      "loss: 0.00602269406323272\n",
      "epochs: 98\n",
      "loss: 0.005920819103734696\n",
      "epochs: 99\n",
      "loss: 0.005823891807827948\n",
      "epochs: 100\n",
      "loss: 0.005728972905397537\n",
      "epochs: 101\n",
      "loss: 0.00562787356694763\n",
      "epochs: 102\n",
      "loss: 0.005538353101573694\n",
      "epochs: 103\n",
      "loss: 0.005450792798349151\n",
      "epochs: 104\n",
      "loss: 0.005360059803363252\n",
      "epochs: 105\n",
      "loss: 0.005282883344850889\n",
      "epochs: 106\n",
      "loss: 0.005198120077993713\n",
      "epochs: 107\n",
      "loss: 0.005118395305033124\n",
      "epochs: 108\n",
      "loss: 0.005044443449770374\n",
      "epochs: 109\n",
      "loss: 0.0049658313061601645\n",
      "epochs: 110\n",
      "loss: 0.004897197621728535\n",
      "epochs: 111\n",
      "loss: 0.0048311297552810824\n",
      "epochs: 112\n",
      "loss: 0.004759241628400063\n",
      "epochs: 113\n",
      "loss: 0.004699059422756964\n",
      "epochs: 114\n",
      "loss: 0.004636844978222562\n",
      "epochs: 115\n",
      "loss: 0.004575157065055552\n",
      "epochs: 116\n",
      "loss: 0.0045179176060614855\n",
      "epochs: 117\n",
      "loss: 0.004462062106272563\n",
      "epochs: 118\n",
      "loss: 0.004405448865966286\n",
      "epochs: 119\n",
      "loss: 0.004352363831862725\n",
      "epochs: 120\n",
      "loss: 0.004301659813669993\n",
      "epochs: 121\n",
      "loss: 0.004251679108290363\n",
      "epochs: 122\n",
      "loss: 0.004198781155959676\n",
      "epochs: 123\n",
      "loss: 0.004152685043273602\n",
      "epochs: 124\n",
      "loss: 0.004106661936503444\n",
      "epochs: 125\n",
      "loss: 0.004061458321536488\n",
      "epochs: 126\n",
      "loss: 0.00401614872520938\n",
      "epochs: 127\n",
      "loss: 0.0039711840091118935\n",
      "epochs: 128\n",
      "loss: 0.003929032682518858\n",
      "epochs: 129\n",
      "loss: 0.0038882403018457626\n",
      "epochs: 130\n",
      "loss: 0.003847359050023066\n",
      "epochs: 131\n",
      "loss: 0.0038048743362236923\n",
      "epochs: 132\n",
      "loss: 0.003764744437843997\n",
      "epochs: 133\n",
      "loss: 0.0037264909055520694\n",
      "epochs: 134\n",
      "loss: 0.0036859470710687036\n",
      "epochs: 135\n",
      "loss: 0.003646485046509907\n",
      "epochs: 136\n",
      "loss: 0.0036055348242441405\n",
      "epochs: 137\n",
      "loss: 0.0035663654627514623\n",
      "epochs: 138\n",
      "loss: 0.0035230919346301084\n",
      "epochs: 139\n",
      "loss: 0.003479896305129237\n",
      "epochs: 140\n",
      "loss: 0.0034325584056312647\n",
      "epochs: 141\n",
      "loss: 0.0033826063473374646\n",
      "epochs: 142\n",
      "loss: 0.0033308824461729704\n",
      "epochs: 143\n",
      "loss: 0.003275900936770245\n",
      "epochs: 144\n",
      "loss: 0.0032259121373993814\n",
      "epochs: 145\n",
      "loss: 0.0031796193675940436\n",
      "epochs: 146\n",
      "loss: 0.0031365910875805554\n",
      "epochs: 147\n",
      "loss: 0.0030985905676711232\n",
      "epochs: 148\n",
      "loss: 0.0030644624444567023\n",
      "epochs: 149\n",
      "loss: 0.003030553120790282\n",
      "epochs: 150\n",
      "loss: 0.0030003107621128567\n",
      "epochs: 151\n",
      "loss: 0.0029687391410877766\n",
      "epochs: 152\n",
      "loss: 0.002941902892927895\n",
      "epochs: 153\n",
      "loss: 0.0029132140640456164\n",
      "epochs: 154\n",
      "loss: 0.0028877680767296168\n",
      "epochs: 155\n",
      "loss: 0.002863804582535316\n",
      "epochs: 156\n",
      "loss: 0.0028388155370263567\n",
      "epochs: 157\n",
      "loss: 0.0028145252926006035\n",
      "epochs: 158\n",
      "loss: 0.002791380867321301\n",
      "epochs: 159\n",
      "loss: 0.0027699722646263507\n",
      "epochs: 160\n",
      "loss: 0.0027493347004275246\n",
      "epochs: 161\n",
      "loss: 0.0027257683470188615\n",
      "epochs: 162\n",
      "loss: 0.002708363262478432\n",
      "epochs: 163\n",
      "loss: 0.002685694763157873\n",
      "epochs: 164\n",
      "loss: 0.0026680032899984705\n",
      "epochs: 165\n",
      "loss: 0.00264802244281013\n",
      "epochs: 166\n",
      "loss: 0.0026305463857521417\n",
      "epochs: 167\n",
      "loss: 0.0026116100494779394\n",
      "epochs: 168\n",
      "loss: 0.0025947656140996784\n",
      "epochs: 169\n",
      "loss: 0.002577534436182942\n",
      "epochs: 170\n",
      "loss: 0.002560181529699831\n",
      "epochs: 171\n",
      "loss: 0.002544595686812687\n",
      "epochs: 172\n",
      "loss: 0.0025277948932005945\n",
      "epochs: 173\n",
      "loss: 0.0025122030223346916\n",
      "epochs: 174\n",
      "loss: 0.002497691294379751\n",
      "epochs: 175\n",
      "loss: 0.002481481689437806\n",
      "epochs: 176\n",
      "loss: 0.0024675824346957107\n",
      "epochs: 177\n",
      "loss: 0.0024526297063277845\n",
      "epochs: 178\n",
      "loss: 0.002439118171792916\n",
      "epochs: 179\n",
      "loss: 0.002425168708406\n",
      "epochs: 180\n",
      "loss: 0.002411869017802449\n",
      "epochs: 181\n",
      "loss: 0.0023993346341905464\n",
      "epochs: 182\n",
      "loss: 0.002385670351219839\n",
      "epochs: 183\n",
      "loss: 0.0023744315589045383\n",
      "epochs: 184\n",
      "loss: 0.00236124436284125\n",
      "epochs: 185\n",
      "loss: 0.002349327613734568\n",
      "epochs: 186\n",
      "loss: 0.0023382408333415938\n",
      "epochs: 187\n",
      "loss: 0.002326255070054632\n",
      "epochs: 188\n",
      "loss: 0.0023149237609908463\n",
      "epochs: 189\n",
      "loss: 0.0023043118851691392\n",
      "epochs: 190\n",
      "loss: 0.002293137121405052\n",
      "epochs: 191\n",
      "loss: 0.0022827808162338035\n",
      "epochs: 192\n",
      "loss: 0.002272334880479961\n",
      "epochs: 193\n",
      "loss: 0.0022621193888327294\n",
      "epochs: 194\n",
      "loss: 0.0022517984108496765\n",
      "epochs: 195\n",
      "loss: 0.00224292490659769\n",
      "epochs: 196\n",
      "loss: 0.0022332946164978175\n",
      "epochs: 197\n",
      "loss: 0.002223119007432006\n",
      "epochs: 198\n",
      "loss: 0.0022151476019370135\n",
      "epochs: 199\n",
      "loss: 0.002205780912638386\n",
      "epochs: 200\n",
      "loss: 0.00219612719905656\n",
      "epochs: 201\n",
      "loss: 0.0021887133359202266\n",
      "epochs: 202\n",
      "loss: 0.002179453382706508\n",
      "epochs: 203\n",
      "loss: 0.0021710038934778138\n",
      "epochs: 204\n",
      "loss: 0.002163296314979354\n",
      "epochs: 205\n",
      "loss: 0.00215430819007814\n",
      "epochs: 206\n",
      "loss: 0.0021467026174910183\n",
      "epochs: 207\n",
      "loss: 0.002139236075142711\n",
      "epochs: 208\n",
      "loss: 0.002131221687543263\n",
      "epochs: 209\n",
      "loss: 0.0021230430345928163\n",
      "epochs: 210\n",
      "loss: 0.0021162010520814085\n",
      "epochs: 211\n",
      "loss: 0.00210850796565651\n",
      "epochs: 212\n",
      "loss: 0.0021009392651985914\n",
      "epochs: 213\n",
      "loss: 0.0020939681245549916\n",
      "epochs: 214\n",
      "loss: 0.002086472606156887\n",
      "epochs: 215\n",
      "loss: 0.0020797840229117705\n",
      "epochs: 216\n",
      "loss: 0.002072509783224713\n",
      "epochs: 217\n",
      "loss: 0.0020655685395160376\n",
      "epochs: 218\n",
      "loss: 0.0020590252349521575\n",
      "epochs: 219\n",
      "loss: 0.002051615795719438\n",
      "epochs: 220\n",
      "loss: 0.0020456562585234126\n",
      "epochs: 221\n",
      "loss: 0.0020385704770997104\n",
      "epochs: 222\n",
      "loss: 0.0020315772427607613\n",
      "epochs: 223\n",
      "loss: 0.002025427822370709\n",
      "epochs: 224\n",
      "loss: 0.002018287436680059\n",
      "epochs: 225\n",
      "loss: 0.0020119285419953107\n",
      "epochs: 226\n",
      "loss: 0.0020052920935511817\n",
      "epochs: 227\n",
      "loss: 0.0019980169082831818\n",
      "epochs: 228\n",
      "loss: 0.001991386191999331\n",
      "epochs: 229\n",
      "loss: 0.00198415082301268\n",
      "epochs: 230\n",
      "loss: 0.0019771139993763802\n",
      "epochs: 231\n",
      "loss: 0.001969092645292617\n",
      "epochs: 232\n",
      "loss: 0.0019612496399814398\n",
      "epochs: 233\n",
      "loss: 0.0019527902660969384\n",
      "epochs: 234\n",
      "loss: 0.0019428058491801098\n",
      "epochs: 235\n",
      "loss: 0.0019321111095017256\n",
      "epochs: 236\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss: 0.0019195216266581598\n",
      "epochs: 237\n",
      "loss: 0.0019044980329216848\n",
      "epochs: 238\n",
      "loss: 0.0018870193762075339\n",
      "epochs: 239\n",
      "loss: 0.001859306300684103\n",
      "epochs: 240\n",
      "loss: 0.0018408325841084879\n",
      "epochs: 241\n",
      "loss: 0.0018217218515399825\n",
      "epochs: 242\n",
      "loss: 0.0018018480064662928\n",
      "epochs: 243\n",
      "loss: 0.0017842051406297074\n",
      "epochs: 244\n",
      "loss: 0.0017655817731001505\n",
      "epochs: 245\n",
      "loss: 0.0017486964205157124\n",
      "epochs: 246\n",
      "loss: 0.0017344910537547044\n",
      "epochs: 247\n",
      "loss: 0.0017209754020708163\n",
      "epochs: 248\n",
      "loss: 0.0017088478067951807\n",
      "epochs: 249\n",
      "loss: 0.0017023518049568856\n",
      "epochs: 250\n",
      "loss: 0.001688531436795331\n",
      "epochs: 251\n",
      "loss: 0.0016794180857255265\n",
      "epochs: 252\n",
      "loss: 0.0016708678384079701\n",
      "epochs: 253\n",
      "loss: 0.0016631152283978527\n",
      "epochs: 254\n",
      "loss: 0.001656128178161149\n",
      "epochs: 255\n",
      "loss: 0.0016483816735758676\n",
      "epochs: 256\n",
      "loss: 0.0016417291729803856\n",
      "epochs: 257\n",
      "loss: 0.0016351323176892407\n",
      "epochs: 258\n",
      "loss: 0.0016290993317900503\n",
      "epochs: 259\n",
      "loss: 0.0016230361183303705\n",
      "epochs: 260\n",
      "loss: 0.0016173530406669995\n",
      "epochs: 261\n",
      "loss: 0.001611213891085099\n",
      "epochs: 262\n",
      "loss: 0.0016060921958136176\n",
      "epochs: 263\n",
      "loss: 0.001600731944593161\n",
      "epochs: 264\n",
      "loss: 0.001595750989524286\n",
      "epochs: 265\n",
      "loss: 0.0015903063935744192\n",
      "epochs: 266\n",
      "loss: 0.001585567300954322\n",
      "epochs: 267\n",
      "loss: 0.0015806735845070015\n",
      "epochs: 268\n",
      "loss: 0.0015760803589730755\n",
      "epochs: 269\n",
      "loss: 0.0015712458349677408\n",
      "epochs: 270\n",
      "loss: 0.0015671493584765422\n",
      "epochs: 271\n",
      "loss: 0.001562322561302532\n",
      "epochs: 272\n",
      "loss: 0.0015582269330093827\n",
      "epochs: 273\n",
      "loss: 0.0015544219175796976\n",
      "epochs: 274\n",
      "loss: 0.0015497981196961892\n",
      "epochs: 275\n",
      "loss: 0.0015457400203872762\n",
      "epochs: 276\n",
      "loss: 0.001541775579287439\n",
      "epochs: 277\n",
      "loss: 0.0015373461923112348\n",
      "epochs: 278\n",
      "loss: 0.0015340471503745142\n",
      "epochs: 279\n",
      "loss: 0.0015298988218236202\n",
      "epochs: 280\n",
      "loss: 0.0015263698724421561\n",
      "epochs: 281\n",
      "loss: 0.001522541101621468\n",
      "epochs: 282\n",
      "loss: 0.0015188591367778688\n",
      "epochs: 283\n",
      "loss: 0.0015153709671135033\n",
      "epochs: 284\n",
      "loss: 0.0015118005681555486\n",
      "epochs: 285\n",
      "loss: 0.0015081190052793948\n",
      "epochs: 286\n",
      "loss: 0.0015048215482985482\n",
      "epochs: 287\n",
      "loss: 0.0015013741268480334\n",
      "epochs: 288\n",
      "loss: 0.0014980546154624904\n",
      "epochs: 289\n",
      "loss: 0.001494656690742651\n",
      "epochs: 290\n",
      "loss: 0.0014912950179206972\n",
      "epochs: 291\n",
      "loss: 0.0014880667923136658\n",
      "epochs: 292\n",
      "loss: 0.0014849989465641796\n",
      "epochs: 293\n",
      "loss: 0.0014818968603211434\n",
      "epochs: 294\n",
      "loss: 0.0014787684890994774\n",
      "epochs: 295\n",
      "loss: 0.0014757287004334904\n",
      "epochs: 296\n",
      "loss: 0.0014726419220863977\n",
      "epochs: 297\n",
      "loss: 0.001469856013325178\n",
      "epochs: 298\n",
      "loss: 0.0014668095076788613\n",
      "epochs: 299\n",
      "loss: 0.0014639500308415813\n",
      "epochs: 300\n",
      "loss: 0.0014609012705660643\n",
      "epochs: 301\n",
      "loss: 0.0014580242345549447\n",
      "epochs: 302\n",
      "loss: 0.0014551951708783848\n",
      "epochs: 303\n",
      "loss: 0.001452499849734895\n",
      "epochs: 304\n",
      "loss: 0.0014496798851323333\n",
      "epochs: 305\n",
      "loss: 0.0014469889679132803\n",
      "epochs: 306\n",
      "loss: 0.0014443131359975657\n",
      "epochs: 307\n",
      "loss: 0.0014416796794304704\n",
      "epochs: 308\n",
      "loss: 0.001439011455995132\n",
      "epochs: 309\n",
      "loss: 0.0014364004908642184\n",
      "epochs: 310\n",
      "loss: 0.00143385659726063\n",
      "epochs: 311\n",
      "loss: 0.0014314719959668432\n",
      "epochs: 312\n",
      "loss: 0.0014288955676981226\n",
      "epochs: 313\n",
      "loss: 0.0014264174848995347\n",
      "epochs: 314\n",
      "loss: 0.00142397474710015\n",
      "epochs: 315\n",
      "loss: 0.0014217141695190086\n",
      "epochs: 316\n",
      "loss: 0.0014192120265094758\n",
      "epochs: 317\n",
      "loss: 0.0014168875199564227\n",
      "epochs: 318\n",
      "loss: 0.0014146528723409562\n",
      "epochs: 319\n",
      "loss: 0.0014123905578286207\n",
      "epochs: 320\n",
      "loss: 0.0014100643243179659\n",
      "epochs: 321\n",
      "loss: 0.0014078453348375878\n",
      "epochs: 322\n",
      "loss: 0.0014055441736445372\n",
      "epochs: 323\n",
      "loss: 0.0014034094606885832\n",
      "epochs: 324\n",
      "loss: 0.0014012630388958497\n",
      "epochs: 325\n",
      "loss: 0.0013990990071595687\n",
      "epochs: 326\n",
      "loss: 0.0013969927453802201\n",
      "epochs: 327\n",
      "loss: 0.0013948841833134739\n",
      "epochs: 328\n",
      "loss: 0.0013929075050664282\n",
      "epochs: 329\n",
      "loss: 0.0013906744856636045\n",
      "epochs: 330\n",
      "loss: 0.0013888057479207634\n",
      "epochs: 331\n",
      "loss: 0.001386636959246591\n",
      "epochs: 332\n",
      "loss: 0.0013848222351629568\n",
      "epochs: 333\n",
      "loss: 0.0013827751038750318\n",
      "epochs: 334\n",
      "loss: 0.0013808320827506462\n",
      "epochs: 335\n",
      "loss: 0.001378893909679638\n",
      "epochs: 336\n",
      "loss: 0.0013770204297885666\n",
      "epochs: 337\n",
      "loss: 0.0013750666533581726\n",
      "epochs: 338\n",
      "loss: 0.001373227288723796\n",
      "epochs: 339\n",
      "loss: 0.0013713748611430927\n",
      "epochs: 340\n",
      "loss: 0.0013695626335893316\n",
      "epochs: 341\n",
      "loss: 0.0013676958417411476\n",
      "epochs: 342\n",
      "loss: 0.00136592483877687\n",
      "epochs: 343\n",
      "loss: 0.0013640711402729585\n",
      "epochs: 344\n",
      "loss: 0.0013624108142935472\n",
      "epochs: 345\n",
      "loss: 0.0013605530214345584\n",
      "epochs: 346\n",
      "loss: 0.0013588795001216317\n",
      "epochs: 347\n",
      "loss: 0.001357173149822014\n",
      "epochs: 348\n",
      "loss: 0.0013554489702051569\n",
      "epochs: 349\n",
      "loss: 0.0013537425006080875\n",
      "epochs: 350\n",
      "loss: 0.0013520550265908385\n",
      "epochs: 351\n",
      "loss: 0.001350463315853028\n",
      "epochs: 352\n",
      "loss: 0.001348723275464853\n",
      "epochs: 353\n",
      "loss: 0.0013472104872940206\n",
      "epochs: 354\n",
      "loss: 0.0013454979797371733\n",
      "epochs: 355\n",
      "loss: 0.0013439712666486424\n",
      "epochs: 356\n",
      "loss: 0.0013423095835634602\n",
      "epochs: 357\n",
      "loss: 0.0013408225653401978\n",
      "epochs: 358\n",
      "loss: 0.0013391668714997816\n",
      "epochs: 359\n",
      "loss: 0.0013377400292959544\n",
      "epochs: 360\n",
      "loss: 0.0013361941565587655\n",
      "epochs: 361\n",
      "loss: 0.0013346514559831185\n",
      "epochs: 362\n",
      "loss: 0.0013331869659204479\n",
      "epochs: 363\n",
      "loss: 0.0013316759824735897\n",
      "epochs: 364\n",
      "loss: 0.0013301496500921026\n",
      "epochs: 365\n",
      "loss: 0.0013287859251385934\n",
      "epochs: 366\n",
      "loss: 0.001327247825996659\n",
      "epochs: 367\n",
      "loss: 0.0013259151709727104\n",
      "epochs: 368\n",
      "loss: 0.001324441453436414\n",
      "epochs: 369\n",
      "loss: 0.0013230651503534224\n",
      "epochs: 370\n",
      "loss: 0.0013215878590305992\n",
      "epochs: 371\n",
      "loss: 0.0013203200645822896\n",
      "epochs: 372\n",
      "loss: 0.0013188492395323683\n",
      "epochs: 373\n",
      "loss: 0.001317505579939035\n",
      "epochs: 374\n",
      "loss: 0.001316141634474598\n",
      "epochs: 375\n",
      "loss: 0.001314841157182196\n",
      "epochs: 376\n",
      "loss: 0.001313494712806598\n",
      "epochs: 377\n",
      "loss: 0.0013121800493310354\n",
      "epochs: 378\n",
      "loss: 0.0013108711734790104\n",
      "epochs: 379\n",
      "loss: 0.0013095730989674589\n",
      "epochs: 380\n",
      "loss: 0.0013083012887459258\n",
      "epochs: 381\n",
      "loss: 0.0013070474472173961\n",
      "epochs: 382\n",
      "loss: 0.0013057732158322413\n",
      "epochs: 383\n",
      "loss: 0.0013045287458047989\n",
      "epochs: 384\n",
      "loss: 0.0013032451273575392\n",
      "epochs: 385\n",
      "loss: 0.0013020539135606187\n",
      "epochs: 386\n",
      "loss: 0.0013008460508698056\n",
      "epochs: 387\n",
      "loss: 0.0012996476237977052\n",
      "epochs: 388\n",
      "loss: 0.0012984321323026222\n",
      "epochs: 389\n",
      "loss: 0.00129729136349437\n",
      "epochs: 390\n",
      "loss: 0.0012960054736001257\n",
      "epochs: 391\n",
      "loss: 0.001294863666336685\n",
      "epochs: 392\n",
      "loss: 0.0012937805678526213\n",
      "epochs: 393\n",
      "loss: 0.00129254842390856\n",
      "epochs: 394\n",
      "loss: 0.0012914275955879797\n",
      "epochs: 395\n",
      "loss: 0.0012903202101923231\n",
      "epochs: 396\n",
      "loss: 0.0012891833068225651\n",
      "epochs: 397\n",
      "loss: 0.001288084562803372\n",
      "epochs: 398\n",
      "loss: 0.0012869678709190244\n",
      "epochs: 399\n",
      "loss: 0.0012858860351946302\n",
      "epochs: 400\n",
      "loss: 0.001284761720242394\n",
      "epochs: 401\n",
      "loss: 0.0012836407443429698\n",
      "epochs: 402\n",
      "loss: 0.0012826591130054125\n",
      "epochs: 403\n",
      "loss: 0.00128154245365962\n",
      "epochs: 404\n",
      "loss: 0.0012805248043262043\n",
      "epochs: 405\n",
      "loss: 0.0012794312808708922\n",
      "epochs: 406\n",
      "loss: 0.0012784323192191161\n",
      "epochs: 407\n",
      "loss: 0.0012773721362319907\n",
      "epochs: 408\n",
      "loss: 0.0012763339145074636\n",
      "epochs: 409\n",
      "loss: 0.0012752728086039243\n",
      "epochs: 410\n",
      "loss: 0.001274310514732112\n",
      "epochs: 411\n",
      "loss: 0.0012732924163891305\n",
      "epochs: 412\n",
      "loss: 0.0012723029831853124\n",
      "epochs: 413\n",
      "loss: 0.0012713274258821866\n",
      "epochs: 414\n",
      "loss: 0.001270256138558731\n",
      "epochs: 415\n",
      "loss: 0.0012693195357343383\n",
      "epochs: 416\n",
      "loss: 0.0012683633659662833\n",
      "epochs: 417\n",
      "loss: 0.0012673825193461167\n",
      "epochs: 418\n",
      "loss: 0.0012664114437591304\n",
      "epochs: 419\n",
      "loss: 0.0012654330131282505\n",
      "epochs: 420\n",
      "loss: 0.0012645722826870138\n",
      "epochs: 421\n",
      "loss: 0.001263602343437067\n",
      "epochs: 422\n",
      "loss: 0.0012626385115607726\n",
      "epochs: 423\n",
      "loss: 0.0012617258518859227\n",
      "epochs: 424\n",
      "loss: 0.001260828038920792\n",
      "epochs: 425\n",
      "loss: 0.0012598896217357722\n",
      "epochs: 426\n",
      "loss: 0.0012590297712848175\n",
      "epochs: 427\n",
      "loss: 0.001258080250083958\n",
      "epochs: 428\n",
      "loss: 0.0012572318986135587\n",
      "epochs: 429\n",
      "loss: 0.0012563014136554365\n",
      "epochs: 430\n",
      "loss: 0.0012554747517323917\n",
      "epochs: 431\n",
      "loss: 0.0012546118091203507\n",
      "epochs: 432\n",
      "loss: 0.001253716918210683\n",
      "epochs: 433\n",
      "loss: 0.001252851958836218\n",
      "epochs: 434\n",
      "loss: 0.001252052739449559\n",
      "epochs: 435\n",
      "loss: 0.0012511890274488114\n",
      "epochs: 436\n",
      "loss: 0.0012503253668270516\n",
      "epochs: 437\n",
      "loss: 0.0012495061021583701\n",
      "epochs: 438\n",
      "loss: 0.001248711081716375\n",
      "epochs: 439\n",
      "loss: 0.0012478576488633467\n",
      "epochs: 440\n",
      "loss: 0.001247025024337465\n",
      "epochs: 441\n",
      "loss: 0.0012462079513023956\n",
      "epochs: 442\n",
      "loss: 0.0012454384707177024\n",
      "epochs: 443\n",
      "loss: 0.0012446289985331572\n",
      "epochs: 444\n",
      "loss: 0.0012438374085655566\n",
      "epochs: 445\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss: 0.0012430665594860288\n",
      "epochs: 446\n",
      "loss: 0.0012422281563656205\n",
      "epochs: 447\n",
      "loss: 0.0012414875182295661\n",
      "epochs: 448\n",
      "loss: 0.0012407235561162342\n",
      "epochs: 449\n",
      "loss: 0.0012399379156235065\n",
      "epochs: 450\n",
      "loss: 0.0012391680246130797\n",
      "epochs: 451\n",
      "loss: 0.0012384340839520565\n",
      "epochs: 452\n",
      "loss: 0.0012376877350722632\n",
      "epochs: 453\n",
      "loss: 0.0012369264608997552\n",
      "epochs: 454\n",
      "loss: 0.001236194909329882\n",
      "epochs: 455\n",
      "loss: 0.0012354228316783739\n",
      "epochs: 456\n",
      "loss: 0.0012347406518470531\n",
      "epochs: 457\n",
      "loss: 0.001234004812518427\n",
      "epochs: 458\n",
      "loss: 0.0012332462146539514\n",
      "epochs: 459\n",
      "loss: 0.0012325523106882401\n",
      "epochs: 460\n",
      "loss: 0.0012318679756681617\n",
      "epochs: 461\n",
      "loss: 0.0012311418086082746\n",
      "epochs: 462\n",
      "loss: 0.0012303956663996054\n",
      "epochs: 463\n",
      "loss: 0.0012297291620911612\n",
      "epochs: 464\n",
      "loss: 0.0012290369180658265\n",
      "epochs: 465\n",
      "loss: 0.0012283454449631298\n",
      "epochs: 466\n",
      "loss: 0.0012276060774988017\n",
      "epochs: 467\n",
      "loss: 0.0012270133429491065\n",
      "epochs: 468\n",
      "loss: 0.00122627252542903\n",
      "epochs: 469\n",
      "loss: 0.0012256212076157999\n",
      "epochs: 470\n",
      "loss: 0.0012249365071410278\n",
      "epochs: 471\n",
      "loss: 0.0012242643360476616\n",
      "epochs: 472\n",
      "loss: 0.0012236011082456073\n",
      "epochs: 473\n",
      "loss: 0.0012229636738882292\n",
      "epochs: 474\n",
      "loss: 0.0012222702920550053\n",
      "epochs: 475\n",
      "loss: 0.0012216689045713952\n",
      "epochs: 476\n",
      "loss: 0.0012209808255275066\n",
      "epochs: 477\n",
      "loss: 0.0012203563046982672\n",
      "epochs: 478\n",
      "loss: 0.001219730199519583\n",
      "epochs: 479\n",
      "loss: 0.001219072211606565\n",
      "epochs: 480\n",
      "loss: 0.0012184492541854797\n",
      "epochs: 481\n",
      "loss: 0.0012178347655667859\n",
      "epochs: 482\n",
      "loss: 0.0012172089578493563\n",
      "epochs: 483\n",
      "loss: 0.0012165684799055787\n",
      "epochs: 484\n",
      "loss: 0.001215972827479302\n",
      "epochs: 485\n",
      "loss: 0.0012153600372511778\n",
      "epochs: 486\n",
      "loss: 0.0012147500354855979\n",
      "epochs: 487\n",
      "loss: 0.0012141437288267188\n",
      "epochs: 488\n",
      "loss: 0.0012135440136610534\n",
      "epochs: 489\n",
      "loss: 0.0012129205139461397\n",
      "epochs: 490\n",
      "loss: 0.0012123712752630598\n",
      "epochs: 491\n",
      "loss: 0.0012117275040311704\n",
      "epochs: 492\n",
      "loss: 0.0012111865299550561\n",
      "epochs: 493\n",
      "loss: 0.00121054674490888\n",
      "epochs: 494\n",
      "loss: 0.001210027005648848\n",
      "epochs: 495\n",
      "loss: 0.001209443529811255\n",
      "epochs: 496\n",
      "loss: 0.0012088550277275405\n",
      "epochs: 497\n",
      "loss: 0.0012082689019515243\n",
      "epochs: 498\n",
      "loss: 0.0012077395754784453\n",
      "epochs: 499\n",
      "loss: 0.0012071584238364083\n",
      "epochs: 500\n",
      "loss: 0.0012065714475930646\n",
      "epochs: 501\n",
      "loss: 0.0012060229120950002\n",
      "epochs: 502\n",
      "loss: 0.0012055069978491406\n",
      "epochs: 503\n",
      "loss: 0.0012049463048887203\n",
      "epochs: 504\n",
      "loss: 0.001204367517511736\n",
      "epochs: 505\n",
      "loss: 0.0012038385537376645\n",
      "epochs: 506\n",
      "loss: 0.0012032939526444308\n",
      "epochs: 507\n",
      "loss: 0.0012027739795584674\n",
      "epochs: 508\n",
      "loss: 0.0012021997937699794\n",
      "epochs: 509\n",
      "loss: 0.0012016900777735418\n",
      "epochs: 510\n",
      "loss: 0.0012011568463838633\n",
      "epochs: 511\n",
      "loss: 0.001200642715293957\n",
      "epochs: 512\n",
      "loss: 0.0012000662680220315\n",
      "epochs: 513\n",
      "loss: 0.0011996472489817223\n",
      "epochs: 514\n",
      "loss: 0.001199045119224383\n",
      "epochs: 515\n",
      "loss: 0.0011985797962021333\n",
      "epochs: 516\n",
      "loss: 0.0011980366628733122\n",
      "epochs: 517\n",
      "loss: 0.0011975303007667087\n",
      "epochs: 518\n",
      "loss: 0.0011970059429831056\n",
      "epochs: 519\n",
      "loss: 0.001196547973348328\n",
      "epochs: 520\n",
      "loss: 0.0011960246781247124\n",
      "epochs: 521\n",
      "loss: 0.0011955075311136543\n",
      "epochs: 522\n",
      "loss: 0.0011950308623313874\n",
      "epochs: 523\n",
      "loss: 0.0011945439556520568\n",
      "epochs: 524\n",
      "loss: 0.001194042762525039\n",
      "epochs: 525\n",
      "loss: 0.001193548290552752\n",
      "epochs: 526\n",
      "loss: 0.0011930862942338804\n",
      "epochs: 527\n",
      "loss: 0.001192577406646066\n",
      "epochs: 528\n",
      "loss: 0.0011921028291846018\n",
      "epochs: 529\n",
      "loss: 0.001191603600307258\n",
      "epochs: 530\n",
      "loss: 0.0011911668928147115\n",
      "epochs: 531\n",
      "loss: 0.001190662553463862\n",
      "epochs: 532\n",
      "loss: 0.0011902122272899223\n",
      "epochs: 533\n",
      "loss: 0.001189691273309133\n",
      "epochs: 534\n",
      "loss: 0.0011892806873614393\n",
      "epochs: 535\n",
      "loss: 0.0011887900984572384\n",
      "epochs: 536\n",
      "loss: 0.0011883612928538772\n",
      "epochs: 537\n",
      "loss: 0.0011878765917524734\n",
      "epochs: 538\n",
      "loss: 0.0011874193345576165\n",
      "epochs: 539\n",
      "loss: 0.0011869540690604447\n",
      "epochs: 540\n",
      "loss: 0.0011865299837710045\n",
      "epochs: 541\n",
      "loss: 0.0011860782999189544\n",
      "epochs: 542\n",
      "loss: 0.0011856011407529628\n",
      "epochs: 543\n",
      "loss: 0.0011851846591703242\n",
      "epochs: 544\n",
      "loss: 0.0011847270493742295\n",
      "epochs: 545\n",
      "loss: 0.0011842924535178272\n",
      "epochs: 546\n",
      "loss: 0.0011838335501417917\n",
      "epochs: 547\n",
      "loss: 0.0011834379014385818\n",
      "epochs: 548\n",
      "loss: 0.0011829663581653278\n",
      "epochs: 549\n",
      "loss: 0.0011825664823429199\n",
      "epochs: 550\n",
      "loss: 0.0011821112194022397\n",
      "epochs: 551\n",
      "loss: 0.0011817041986055257\n",
      "epochs: 552\n",
      "loss: 0.001181255868280975\n",
      "epochs: 553\n",
      "loss: 0.0011808757072072327\n",
      "epochs: 554\n",
      "loss: 0.0011804095518698574\n",
      "epochs: 555\n",
      "loss: 0.001180009381574014\n",
      "epochs: 556\n",
      "loss: 0.0011795888232052354\n",
      "epochs: 557\n",
      "loss: 0.001179177344764877\n",
      "epochs: 558\n",
      "loss: 0.0011787602926660087\n",
      "epochs: 559\n",
      "loss: 0.001178339310009774\n",
      "epochs: 560\n",
      "loss: 0.001177938063344273\n",
      "epochs: 561\n",
      "loss: 0.0011775196935485518\n",
      "epochs: 562\n",
      "loss: 0.0011771175933165845\n",
      "epochs: 563\n",
      "loss: 0.001176703498841133\n",
      "epochs: 564\n",
      "loss: 0.0011763123898592429\n",
      "epochs: 565\n",
      "loss: 0.0011758932847910203\n",
      "epochs: 566\n",
      "loss: 0.0011755180151942271\n",
      "epochs: 567\n",
      "loss: 0.0011750882185468528\n",
      "epochs: 568\n",
      "loss: 0.0011747412182691328\n",
      "epochs: 569\n",
      "loss: 0.0011743092012437045\n",
      "epochs: 570\n",
      "loss: 0.001173937384974665\n",
      "epochs: 571\n",
      "loss: 0.0011735415169469693\n",
      "epochs: 572\n",
      "loss: 0.0011731473783616736\n",
      "epochs: 573\n",
      "loss: 0.00117277001833738\n",
      "epochs: 574\n",
      "loss: 0.0011723805707471414\n",
      "epochs: 575\n",
      "loss: 0.0011720024276106289\n",
      "epochs: 576\n",
      "loss: 0.0011716175470832025\n",
      "epochs: 577\n",
      "loss: 0.0011712689597720158\n",
      "epochs: 578\n",
      "loss: 0.0011708593600227339\n",
      "epochs: 579\n",
      "loss: 0.001170500130142742\n",
      "epochs: 580\n",
      "loss: 0.0011701278484944008\n",
      "epochs: 581\n",
      "loss: 0.0011697495358734603\n",
      "epochs: 582\n",
      "loss: 0.001169379956761294\n",
      "epochs: 583\n",
      "loss: 0.0011690270368564727\n",
      "epochs: 584\n",
      "loss: 0.0011686465290286956\n",
      "epochs: 585\n",
      "loss: 0.0011682748398381057\n",
      "epochs: 586\n",
      "loss: 0.0011679373960999532\n",
      "epochs: 587\n",
      "loss: 0.0011675678820855819\n",
      "epochs: 588\n",
      "loss: 0.0011672145666448033\n",
      "epochs: 589\n",
      "loss: 0.0011668362857204925\n",
      "epochs: 590\n",
      "loss: 0.0011665251060987956\n",
      "epochs: 591\n",
      "loss: 0.0011661317290783233\n",
      "epochs: 592\n",
      "loss: 0.0011658107760697419\n",
      "epochs: 593\n",
      "loss: 0.001165436885209799\n",
      "epochs: 594\n",
      "loss: 0.001165097896557974\n",
      "epochs: 595\n",
      "loss: 0.0011647471519880723\n",
      "epochs: 596\n",
      "loss: 0.001164405304795343\n",
      "epochs: 597\n",
      "loss: 0.001164066883738842\n",
      "epochs: 598\n",
      "loss: 0.0011637043416687903\n",
      "epochs: 599\n",
      "loss: 0.0011633756028181226\n",
      "epochs: 600\n",
      "loss: 0.001163041436257373\n",
      "epochs: 601\n",
      "loss: 0.0011627043053343228\n",
      "epochs: 602\n",
      "loss: 0.0011623489726975639\n",
      "epochs: 603\n",
      "loss: 0.0011620221711379504\n",
      "epochs: 604\n",
      "loss: 0.0011616843985955117\n",
      "epochs: 605\n",
      "loss: 0.0011613716494032061\n",
      "epochs: 606\n",
      "loss: 0.0011610241284057\n",
      "epochs: 607\n",
      "loss: 0.0011606944842267246\n",
      "epochs: 608\n",
      "loss: 0.0011603709225822285\n",
      "epochs: 609\n",
      "loss: 0.0011600552354823837\n",
      "epochs: 610\n",
      "loss: 0.0011597237727581386\n",
      "epochs: 611\n",
      "loss: 0.0011593822538829939\n",
      "epochs: 612\n",
      "loss: 0.0011590994612629775\n",
      "epochs: 613\n",
      "loss: 0.0011587565057829049\n",
      "epochs: 614\n",
      "loss: 0.0011584500352520173\n",
      "epochs: 615\n",
      "loss: 0.0011581166230494025\n",
      "epochs: 616\n",
      "loss: 0.0011578034697768333\n",
      "epochs: 617\n",
      "loss: 0.00115749236074766\n",
      "epochs: 618\n",
      "loss: 0.001157181078542381\n",
      "epochs: 619\n",
      "loss: 0.0011568680191899078\n",
      "epochs: 620\n",
      "loss: 0.0011565426747815884\n",
      "epochs: 621\n",
      "loss: 0.001156256431382889\n",
      "epochs: 622\n",
      "loss: 0.001155929179580522\n",
      "epochs: 623\n",
      "loss: 0.0011556417274528749\n",
      "epochs: 624\n",
      "loss: 0.0011553211142346423\n",
      "epochs: 625\n",
      "loss: 0.0011550191395300136\n",
      "epochs: 626\n",
      "loss: 0.001154714585495004\n",
      "epochs: 627\n",
      "loss: 0.0011544282848343452\n",
      "epochs: 628\n",
      "loss: 0.0011541153717241433\n",
      "epochs: 629\n",
      "loss: 0.0011538160478234904\n",
      "epochs: 630\n",
      "loss: 0.0011535190231875775\n",
      "epochs: 631\n",
      "loss: 0.0011532354666214262\n",
      "epochs: 632\n",
      "loss: 0.001152933007949642\n",
      "epochs: 633\n",
      "loss: 0.0011526303453873552\n",
      "epochs: 634\n",
      "loss: 0.001152342858771988\n",
      "epochs: 635\n",
      "loss: 0.0011520492138612536\n",
      "epochs: 636\n",
      "loss: 0.0011517732169518357\n",
      "epochs: 637\n",
      "loss: 0.0011514815250556014\n",
      "epochs: 638\n",
      "loss: 0.0011511739565030037\n",
      "epochs: 639\n",
      "loss: 0.0011509064380997005\n",
      "epochs: 640\n",
      "loss: 0.001150614494303762\n",
      "epochs: 641\n",
      "loss: 0.001150338872075924\n",
      "epochs: 642\n",
      "loss: 0.001150044098409916\n",
      "epochs: 643\n",
      "loss: 0.001149772935543757\n",
      "epochs: 644\n",
      "loss: 0.001149475018351153\n",
      "epochs: 645\n",
      "loss: 0.0011492248398987232\n",
      "epochs: 646\n",
      "loss: 0.001148921821267144\n",
      "epochs: 647\n",
      "loss: 0.001148659104115767\n",
      "epochs: 648\n",
      "loss: 0.0011483739438331254\n",
      "epochs: 649\n",
      "loss: 0.0011481064380149857\n",
      "epochs: 650\n",
      "loss: 0.0011478239131199192\n",
      "epochs: 651\n",
      "loss: 0.001147560849084019\n",
      "epochs: 652\n",
      "loss: 0.0011472838272093898\n",
      "epochs: 653\n",
      "loss: 0.0011470181525965824\n",
      "epochs: 654\n",
      "loss: 0.001146748123399883\n",
      "epochs: 655\n",
      "loss: 0.0011464739684844203\n",
      "epochs: 656\n",
      "loss: 0.0011462051183378846\n",
      "epochs: 657\n",
      "loss: 0.0011459455133111657\n",
      "epochs: 658\n",
      "loss: 0.001145690904154965\n",
      "epochs: 659\n",
      "loss: 0.0011454245765622386\n",
      "epochs: 660\n",
      "loss: 0.0011451539359715313\n",
      "epochs: 661\n",
      "loss: 0.0011448980107553954\n",
      "epochs: 662\n",
      "loss: 0.0011446446153599093\n",
      "epochs: 663\n",
      "loss: 0.001144384045961416\n",
      "epochs: 664\n",
      "loss: 0.0011441139699763121\n",
      "epochs: 665\n",
      "loss: 0.0011438722587446706\n",
      "epochs: 666\n",
      "loss: 0.001143610016406547\n",
      "epochs: 667\n",
      "loss: 0.0011433604757511897\n",
      "epochs: 668\n",
      "loss: 0.001143102645786762\n",
      "epochs: 669\n",
      "loss: 0.001142853905088742\n",
      "epochs: 670\n",
      "loss: 0.0011425953231872486\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epochs: 671\n",
      "loss: 0.0011423505657119678\n",
      "epochs: 672\n",
      "loss: 0.001142097070514619\n",
      "epochs: 673\n",
      "loss: 0.0011418519425594493\n",
      "epochs: 674\n",
      "loss: 0.0011416142153170975\n",
      "epochs: 675\n",
      "loss: 0.0011413471661559553\n",
      "epochs: 676\n",
      "loss: 0.001141125098390843\n",
      "epochs: 677\n",
      "loss: 0.0011408639151785861\n",
      "epochs: 678\n",
      "loss: 0.00114063043626819\n",
      "epochs: 679\n",
      "loss: 0.0011403784976397955\n",
      "epochs: 680\n",
      "loss: 0.0011401457280660705\n",
      "epochs: 681\n",
      "loss: 0.0011398978010092383\n",
      "epochs: 682\n",
      "loss: 0.001139669643004558\n",
      "epochs: 683\n",
      "loss: 0.0011394250431344538\n",
      "epochs: 684\n",
      "loss: 0.001139182410735846\n",
      "epochs: 685\n",
      "loss: 0.0011389466964530496\n",
      "epochs: 686\n",
      "loss: 0.001138712176924153\n",
      "epochs: 687\n",
      "loss: 0.0011384823337382808\n",
      "epochs: 688\n",
      "loss: 0.0011382478227923971\n",
      "epochs: 689\n",
      "loss: 0.0011380084515515614\n",
      "epochs: 690\n",
      "loss: 0.0011377824729951747\n",
      "epochs: 691\n",
      "loss: 0.0011375406102348022\n",
      "epochs: 692\n",
      "loss: 0.001137316761014617\n",
      "epochs: 693\n",
      "loss: 0.0011370798296382841\n",
      "epochs: 694\n",
      "loss: 0.0011368586020063173\n",
      "epochs: 695\n",
      "loss: 0.0011366177869587015\n",
      "epochs: 696\n",
      "loss: 0.0011364121917521203\n",
      "epochs: 697\n",
      "loss: 0.0011361722251859356\n",
      "epochs: 698\n",
      "loss: 0.0011359564178866754\n",
      "epochs: 699\n",
      "loss: 0.001135732702741259\n",
      "epochs: 700\n",
      "loss: 0.0011355014035866532\n",
      "epochs: 701\n",
      "loss: 0.0011352770775320505\n",
      "epochs: 702\n",
      "loss: 0.0011350583178231534\n",
      "epochs: 703\n",
      "loss: 0.0011348391495780614\n",
      "epochs: 704\n",
      "loss: 0.001134612052121171\n",
      "epochs: 705\n",
      "loss: 0.0011343984983063363\n",
      "epochs: 706\n",
      "loss: 0.0011341808600690536\n",
      "epochs: 707\n",
      "loss: 0.0011339636450552087\n",
      "epochs: 708\n",
      "loss: 0.0011337453230594182\n",
      "epochs: 709\n",
      "loss: 0.0011335232710059924\n",
      "epochs: 710\n",
      "loss: 0.0011333097344267647\n",
      "epochs: 711\n",
      "loss: 0.0011330946842040236\n",
      "epochs: 712\n",
      "loss: 0.0011328939091102394\n",
      "epochs: 713\n",
      "loss: 0.0011326688293329717\n",
      "epochs: 714\n",
      "loss: 0.0011324605462536886\n",
      "epochs: 715\n",
      "loss: 0.001132246748717526\n",
      "epochs: 716\n",
      "loss: 0.0011320397238864487\n",
      "epochs: 717\n",
      "loss: 0.0011318332710439057\n",
      "epochs: 718\n",
      "loss: 0.0011316127231672858\n",
      "epochs: 719\n",
      "loss: 0.001131417523992617\n",
      "epochs: 720\n",
      "loss: 0.0011311985231072016\n",
      "epochs: 721\n",
      "loss: 0.0011309952474642254\n",
      "epochs: 722\n",
      "loss: 0.0011307899514801354\n",
      "epochs: 723\n",
      "loss: 0.0011305927202962653\n",
      "epochs: 724\n",
      "loss: 0.0011303847898779982\n",
      "epochs: 725\n",
      "loss: 0.0011301783319120477\n",
      "epochs: 726\n",
      "loss: 0.0011299762628561294\n",
      "epochs: 727\n",
      "loss: 0.0011297802469741652\n",
      "epochs: 728\n",
      "loss: 0.0011295781106906387\n",
      "epochs: 729\n",
      "loss: 0.0011293704961005061\n",
      "epochs: 730\n",
      "loss: 0.0011291692028071686\n",
      "epochs: 731\n",
      "loss: 0.001128974309706712\n",
      "epochs: 732\n",
      "loss: 0.0011287824416971095\n",
      "epochs: 733\n",
      "loss: 0.0011285846754418346\n",
      "epochs: 734\n",
      "loss: 0.001128371911657635\n",
      "epochs: 735\n",
      "loss: 0.001128192763184917\n",
      "epochs: 736\n",
      "loss: 0.0011279879336238672\n",
      "epochs: 737\n",
      "loss: 0.001127810466289633\n",
      "epochs: 738\n",
      "loss: 0.0011275919580187195\n",
      "epochs: 739\n",
      "loss: 0.0011274224288177273\n",
      "epochs: 740\n",
      "loss: 0.00112721523053345\n",
      "epochs: 741\n",
      "loss: 0.001127027833501775\n",
      "epochs: 742\n",
      "loss: 0.0011268362436854381\n",
      "epochs: 743\n",
      "loss: 0.0011266468710935283\n",
      "epochs: 744\n",
      "loss: 0.001126450512203346\n",
      "epochs: 745\n",
      "loss: 0.0011262624628999964\n",
      "epochs: 746\n",
      "loss: 0.001126082473017905\n",
      "epochs: 747\n",
      "loss: 0.0011258919855766912\n",
      "epochs: 748\n",
      "loss: 0.0011257045419671532\n",
      "epochs: 749\n",
      "loss: 0.001125513075942178\n",
      "epochs: 750\n",
      "loss: 0.0011253327743270816\n",
      "epochs: 751\n",
      "loss: 0.001125152792047378\n",
      "epochs: 752\n",
      "loss: 0.0011249525099389522\n",
      "epochs: 753\n",
      "loss: 0.0011247816494045506\n",
      "epochs: 754\n",
      "loss: 0.001124590066353337\n",
      "epochs: 755\n",
      "loss: 0.0011244135447728372\n",
      "epochs: 756\n",
      "loss: 0.0011242312051939624\n",
      "epochs: 757\n",
      "loss: 0.0011240492698014477\n",
      "epochs: 758\n",
      "loss: 0.0011238689626112918\n",
      "epochs: 759\n",
      "loss: 0.0011236911881560136\n",
      "epochs: 760\n",
      "loss: 0.0011235124488268337\n",
      "epochs: 761\n",
      "loss: 0.0011233287352716288\n",
      "epochs: 762\n",
      "loss: 0.001123156452854595\n",
      "epochs: 763\n",
      "loss: 0.0011229727351870005\n",
      "epochs: 764\n",
      "loss: 0.0011228087825887242\n",
      "epochs: 765\n",
      "loss: 0.0011226242146722856\n",
      "epochs: 766\n",
      "loss: 0.001122439665399685\n",
      "epochs: 767\n",
      "loss: 0.001122279825683484\n",
      "epochs: 768\n",
      "loss: 0.0011220942644235897\n",
      "epochs: 769\n",
      "loss: 0.0011219252510074336\n",
      "epochs: 770\n",
      "loss: 0.0011217509956642018\n",
      "epochs: 771\n",
      "loss: 0.0011215820497110162\n",
      "epochs: 772\n",
      "loss: 0.001121410683440895\n",
      "epochs: 773\n",
      "loss: 0.0011212377128638669\n",
      "epochs: 774\n",
      "loss: 0.0011210657006972484\n",
      "epochs: 775\n",
      "loss: 0.0011208912809731447\n",
      "epochs: 776\n",
      "loss: 0.0011207308792951605\n",
      "epochs: 777\n",
      "loss: 0.0011205499531040696\n",
      "epochs: 778\n",
      "loss: 0.001120388224198702\n",
      "epochs: 779\n",
      "loss: 0.0011202167706398157\n",
      "epochs: 780\n",
      "loss: 0.0011200576345974584\n",
      "epochs: 781\n",
      "loss: 0.001119889633296243\n",
      "epochs: 782\n",
      "loss: 0.0011197152744018132\n",
      "epochs: 783\n",
      "loss: 0.0011195546234507318\n",
      "epochs: 784\n",
      "loss: 0.001119384086070407\n",
      "epochs: 785\n",
      "loss: 0.0011192337974553995\n",
      "epochs: 786\n",
      "loss: 0.001119059875365554\n",
      "epochs: 787\n",
      "loss: 0.0011188963048576115\n",
      "epochs: 788\n",
      "loss: 0.0011187326931279883\n",
      "epochs: 789\n",
      "loss: 0.00111857537899694\n",
      "epochs: 790\n",
      "loss: 0.0011184158267988666\n",
      "epochs: 791\n",
      "loss: 0.0011182402427557827\n",
      "epochs: 792\n",
      "loss: 0.0011180957670082427\n",
      "epochs: 793\n",
      "loss: 0.0011179190585206289\n",
      "epochs: 794\n",
      "loss: 0.0011177786333813505\n",
      "epochs: 795\n",
      "loss: 0.0011176090009034018\n",
      "epochs: 796\n",
      "loss: 0.0011174516952683824\n",
      "epochs: 797\n",
      "loss: 0.0011172889844782874\n",
      "epochs: 798\n",
      "loss: 0.0011171393482275725\n",
      "epochs: 799\n",
      "loss: 0.0011169822148910616\n",
      "epochs: 800\n",
      "loss: 0.0011168195486661095\n",
      "epochs: 801\n",
      "loss: 0.0011166734196092034\n",
      "epochs: 802\n",
      "loss: 0.0011165121088746674\n",
      "epochs: 803\n",
      "loss: 0.0011163564795691515\n",
      "epochs: 804\n",
      "loss: 0.0011162080254789836\n",
      "epochs: 805\n",
      "loss: 0.0011160393622856178\n",
      "epochs: 806\n",
      "loss: 0.0011158980579084026\n",
      "epochs: 807\n",
      "loss: 0.001115741447777927\n",
      "epochs: 808\n",
      "loss: 0.0011155922391691067\n",
      "epochs: 809\n",
      "loss: 0.0011154299243912847\n",
      "epochs: 810\n",
      "loss: 0.0011152907289594235\n",
      "epochs: 811\n",
      "loss: 0.0011151317533868068\n",
      "epochs: 812\n",
      "loss: 0.0011149880420033512\n",
      "epochs: 813\n",
      "loss: 0.001114834778464097\n",
      "epochs: 814\n",
      "loss: 0.0011146811866963237\n",
      "epochs: 815\n",
      "loss: 0.0011145389000103932\n",
      "epochs: 816\n",
      "loss: 0.001114383690286498\n",
      "epochs: 817\n",
      "loss: 0.0011142415611143323\n",
      "epochs: 818\n",
      "loss: 0.0011140931252639963\n",
      "epochs: 819\n",
      "loss: 0.0011139413041880387\n",
      "epochs: 820\n",
      "loss: 0.0011137970908142887\n",
      "epochs: 821\n",
      "loss: 0.0011136495803659048\n",
      "epochs: 822\n",
      "loss: 0.001113511072569401\n",
      "epochs: 823\n",
      "loss: 0.0011133583289410288\n",
      "epochs: 824\n",
      "loss: 0.0011132178979628231\n",
      "epochs: 825\n",
      "loss: 0.0011130718271335027\n",
      "epochs: 826\n",
      "loss: 0.0011129296325592284\n",
      "epochs: 827\n",
      "loss: 0.0011127865343755583\n",
      "epochs: 828\n",
      "loss: 0.0011126361465015808\n",
      "epochs: 829\n",
      "loss: 0.0011125056311401724\n",
      "epochs: 830\n",
      "loss: 0.0011123539070817484\n",
      "epochs: 831\n",
      "loss: 0.0011122221660105215\n",
      "epochs: 832\n",
      "loss: 0.0011120699587533033\n",
      "epochs: 833\n",
      "loss: 0.0011119322275966843\n",
      "epochs: 834\n",
      "loss: 0.0011117935477194633\n",
      "epochs: 835\n",
      "loss: 0.0011116584235201534\n",
      "epochs: 836\n",
      "loss: 0.001111514888840647\n",
      "epochs: 837\n",
      "loss: 0.001111371281258965\n",
      "epochs: 838\n",
      "loss: 0.0011112430080641499\n",
      "epochs: 839\n",
      "loss: 0.0011110990725716782\n",
      "epochs: 840\n",
      "loss: 0.0011109670991736253\n",
      "epochs: 841\n",
      "loss: 0.0011108246199558508\n",
      "epochs: 842\n",
      "loss: 0.0011106848815570693\n",
      "epochs: 843\n",
      "loss: 0.0011105528639070664\n",
      "epochs: 844\n",
      "loss: 0.0011104105361562577\n",
      "epochs: 845\n",
      "loss: 0.0011102846719096132\n",
      "epochs: 846\n",
      "loss: 0.0011101429834332087\n",
      "epochs: 847\n",
      "loss: 0.0011100101263650764\n",
      "epochs: 848\n",
      "loss: 0.0011098730499753157\n",
      "epochs: 849\n",
      "loss: 0.0011097447026132221\n",
      "epochs: 850\n",
      "loss: 0.0011096048145782867\n",
      "epochs: 851\n",
      "loss: 0.0011094687827538617\n",
      "epochs: 852\n",
      "loss: 0.0011093450440213461\n",
      "epochs: 853\n",
      "loss: 0.0011092085295225592\n",
      "epochs: 854\n",
      "loss: 0.00110907866465768\n",
      "epochs: 855\n",
      "loss: 0.001108944972350102\n",
      "epochs: 856\n",
      "loss: 0.0011088119099679444\n",
      "epochs: 857\n",
      "loss: 0.001108683833191985\n",
      "epochs: 858\n",
      "loss: 0.001108546886426695\n",
      "epochs: 859\n",
      "loss: 0.0011084259763378807\n",
      "epochs: 860\n",
      "loss: 0.0011082906897021975\n",
      "epochs: 861\n",
      "loss: 0.0011081610698742994\n",
      "epochs: 862\n",
      "loss: 0.0011080314680200988\n",
      "epochs: 863\n",
      "loss: 0.0011079095163227693\n",
      "epochs: 864\n",
      "loss: 0.00110777422414375\n",
      "epochs: 865\n",
      "loss: 0.0011076469715401197\n",
      "epochs: 866\n",
      "loss: 0.0011075221239731944\n",
      "epochs: 867\n",
      "loss: 0.0011073962650241375\n",
      "epochs: 868\n",
      "loss: 0.0011072689542082868\n",
      "epochs: 869\n",
      "loss: 0.0011071370319508222\n",
      "epochs: 870\n",
      "loss: 0.0011070173453419346\n",
      "epochs: 871\n",
      "loss: 0.00110689030873492\n",
      "epochs: 872\n",
      "loss: 0.0011067647778759697\n",
      "epochs: 873\n",
      "loss: 0.0011066417879468436\n",
      "epochs: 874\n",
      "loss: 0.0011065139801542197\n",
      "epochs: 875\n",
      "loss: 0.0011063973470312716\n",
      "epochs: 876\n",
      "loss: 0.0011062676969729828\n",
      "epochs: 877\n",
      "loss: 0.0011061466165917164\n",
      "epochs: 878\n",
      "loss: 0.0011060253319566148\n",
      "epochs: 879\n",
      "loss: 0.0011058973187992956\n",
      "epochs: 880\n",
      "loss: 0.0011057800399655292\n",
      "epochs: 881\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss: 0.00110565441651449\n",
      "epochs: 882\n",
      "loss: 0.0011055389636390684\n",
      "epochs: 883\n",
      "loss: 0.0011054085856335302\n",
      "epochs: 884\n",
      "loss: 0.0011052961006242845\n",
      "epochs: 885\n",
      "loss: 0.001105173648274932\n",
      "epochs: 886\n",
      "loss: 0.0011050538303893282\n",
      "epochs: 887\n",
      "loss: 0.0011049332990978298\n",
      "epochs: 888\n",
      "loss: 0.001104814071487448\n",
      "epochs: 889\n",
      "loss: 0.0011046921577016317\n",
      "epochs: 890\n",
      "loss: 0.001104576318793185\n",
      "epochs: 891\n",
      "loss: 0.001104462411592882\n",
      "epochs: 892\n",
      "loss: 0.0011043427318222787\n",
      "epochs: 893\n",
      "loss: 0.0011042252426936574\n",
      "epochs: 894\n",
      "loss: 0.0011041103107182716\n",
      "epochs: 895\n",
      "loss: 0.001103982994976512\n",
      "epochs: 896\n",
      "loss: 0.0011038800898882824\n",
      "epochs: 897\n",
      "loss: 0.0011037543144416233\n",
      "epochs: 898\n",
      "loss: 0.0011036423764771248\n",
      "epochs: 899\n",
      "loss: 0.001103526156964253\n",
      "epochs: 900\n",
      "loss: 0.0011034084001269229\n",
      "epochs: 901\n",
      "loss: 0.0011032956899309212\n",
      "epochs: 902\n",
      "loss: 0.001103183388369334\n",
      "epochs: 903\n",
      "loss: 0.001103065100796943\n",
      "epochs: 904\n",
      "loss: 0.001102952065350165\n",
      "epochs: 905\n",
      "loss: 0.0011028389199196794\n",
      "epochs: 906\n",
      "loss: 0.0011027252800571315\n",
      "epochs: 907\n",
      "loss: 0.001102614431414406\n",
      "epochs: 908\n",
      "loss: 0.0011025004206483896\n",
      "epochs: 909\n",
      "loss: 0.0011023849535121493\n",
      "epochs: 910\n",
      "loss: 0.0011022823461185936\n",
      "epochs: 911\n",
      "loss: 0.0011021577690097312\n",
      "epochs: 912\n",
      "loss: 0.0011020584276314356\n",
      "epochs: 913\n",
      "loss: 0.0011019400917194556\n",
      "epochs: 914\n",
      "loss: 0.0011018324443443977\n",
      "epochs: 915\n",
      "loss: 0.0011017240877050827\n",
      "epochs: 916\n",
      "loss: 0.001101609254753719\n",
      "epochs: 917\n",
      "loss: 0.0011014986809196074\n",
      "epochs: 918\n",
      "loss: 0.0011013868627193588\n",
      "epochs: 919\n",
      "loss: 0.0011012855752313463\n",
      "epochs: 920\n",
      "loss: 0.0011011698867301317\n",
      "epochs: 921\n",
      "loss: 0.0011010641255168616\n",
      "epochs: 922\n",
      "loss: 0.0011009547709622668\n",
      "epochs: 923\n",
      "loss: 0.0011008496941887216\n",
      "epochs: 924\n",
      "loss: 0.0011007418380314842\n",
      "epochs: 925\n",
      "loss: 0.0011006291589590288\n",
      "epochs: 926\n",
      "loss: 0.0011005276247124578\n",
      "epochs: 927\n",
      "loss: 0.0011004196449645879\n",
      "epochs: 928\n",
      "loss: 0.0011003102672079658\n",
      "epochs: 929\n",
      "loss: 0.0011002070451642714\n",
      "epochs: 930\n",
      "loss: 0.0011000987209631762\n",
      "epochs: 931\n",
      "loss: 0.0010999964053983197\n",
      "epochs: 932\n",
      "loss: 0.0010998844476874307\n",
      "epochs: 933\n",
      "loss: 0.0010997868024168547\n",
      "epochs: 934\n",
      "loss: 0.0010996765585687494\n",
      "epochs: 935\n",
      "loss: 0.0010995759729264104\n",
      "epochs: 936\n",
      "loss: 0.0010994686648459536\n",
      "epochs: 937\n",
      "loss: 0.0010993638443108252\n",
      "epochs: 938\n",
      "loss: 0.0010992633537388866\n",
      "epochs: 939\n",
      "loss: 0.0010991587778240997\n",
      "epochs: 940\n",
      "loss: 0.0010990564089697191\n",
      "epochs: 941\n",
      "loss: 0.0010989577430170667\n",
      "epochs: 942\n",
      "loss: 0.0010988482316458316\n",
      "epochs: 943\n",
      "loss: 0.0010987489089628842\n",
      "epochs: 944\n",
      "loss: 0.001098644947364296\n",
      "epochs: 945\n",
      "loss: 0.0010985479401390953\n",
      "epochs: 946\n",
      "loss: 0.0010984407227323776\n",
      "epochs: 947\n",
      "loss: 0.0010983484552334326\n",
      "epochs: 948\n",
      "loss: 0.0010982420181893894\n",
      "epochs: 949\n",
      "loss: 0.0010981413928358595\n",
      "epochs: 950\n",
      "loss: 0.0010980422875618591\n",
      "epochs: 951\n",
      "loss: 0.0010979414230998224\n",
      "epochs: 952\n",
      "loss: 0.0010978462963839983\n",
      "epochs: 953\n",
      "loss: 0.0010977372182129277\n",
      "epochs: 954\n",
      "loss: 0.0010976480202831206\n",
      "epochs: 955\n",
      "loss: 0.0010975428286151292\n",
      "epochs: 956\n",
      "loss: 0.0010974468540027981\n",
      "epochs: 957\n",
      "loss: 0.0010973464837256066\n",
      "epochs: 958\n",
      "loss: 0.0010972444913461476\n",
      "epochs: 959\n",
      "loss: 0.0010971552706367724\n",
      "epochs: 960\n",
      "loss: 0.0010970514401163222\n",
      "epochs: 961\n",
      "loss: 0.0010969553162609822\n",
      "epochs: 962\n",
      "loss: 0.0010968599854718553\n",
      "epochs: 963\n",
      "loss: 0.001096758812989964\n",
      "epochs: 964\n",
      "loss: 0.001096667717419678\n",
      "epochs: 965\n",
      "loss: 0.0010965615359713214\n",
      "epochs: 966\n",
      "loss: 0.0010964712838586492\n",
      "epochs: 967\n",
      "loss: 0.0010963732954615932\n",
      "epochs: 968\n",
      "loss: 0.0010962834826437557\n",
      "epochs: 969\n",
      "loss: 0.001096182256217576\n",
      "epochs: 970\n",
      "loss: 0.0010960850287216842\n",
      "epochs: 971\n",
      "loss: 0.001095993338952298\n",
      "epochs: 972\n",
      "loss: 0.0010958954740921507\n",
      "epochs: 973\n",
      "loss: 0.0010958045984886108\n",
      "epochs: 974\n",
      "loss: 0.0010957093531537695\n",
      "epochs: 975\n",
      "loss: 0.0010956099367274274\n",
      "epochs: 976\n",
      "loss: 0.0010955237026452269\n",
      "epochs: 977\n",
      "loss: 0.0010954249352652945\n",
      "epochs: 978\n",
      "loss: 0.001095335120832099\n",
      "epochs: 979\n",
      "loss: 0.001095238314830888\n",
      "epochs: 980\n",
      "loss: 0.0010951484421065309\n",
      "epochs: 981\n",
      "loss: 0.001095054182888672\n",
      "epochs: 982\n",
      "loss: 0.0010949642118787918\n",
      "epochs: 983\n",
      "loss: 0.0010948700631164684\n",
      "epochs: 984\n",
      "loss: 0.0010947782508308006\n",
      "epochs: 985\n",
      "loss: 0.0010946903630499277\n",
      "epochs: 986\n",
      "loss: 0.0010945901720911032\n",
      "epochs: 987\n",
      "loss: 0.0010945083379469906\n",
      "epochs: 988\n",
      "loss: 0.0010944115399628745\n",
      "epochs: 989\n",
      "loss: 0.0010943226180221523\n",
      "epochs: 990\n",
      "loss: 0.0010942343875136216\n",
      "epochs: 991\n",
      "loss: 0.0010941382421306807\n",
      "epochs: 992\n",
      "loss: 0.0010940502687090267\n",
      "epochs: 993\n",
      "loss: 0.0010939623734544008\n",
      "epochs: 994\n",
      "loss: 0.0010938739655005096\n",
      "epochs: 995\n",
      "loss: 0.0010937820239679923\n",
      "epochs: 996\n",
      "loss: 0.0010936946991888885\n",
      "epochs: 997\n",
      "loss: 0.001093608018421182\n",
      "epochs: 998\n",
      "loss: 0.0010935119368382874\n",
      "epochs: 999\n",
      "loss: 0.0010934314915469513\n"
     ]
    }
   ],
   "source": [
    "ann = ANN([30, 10], batch_size = 256, epochs=1000, learning_rate=1,batch_normal=False)\n",
    "ann.fit(train_x, train_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 359,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy: 0.9158249158249159\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.9158249158249159"
      ]
     },
     "execution_count": 359,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y2 = ann.predict(test_x)\n",
    "ann.accuracy(y2, test_y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 测试"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 416,
   "metadata": {},
   "outputs": [],
   "source": [
    "#多层神经网络\n",
    "from sklearn.preprocessing import StandardScaler \n",
    "def multiplilyNet(learning_rate = 1):\n",
    "    digits = load_digits()\n",
    "    data = digits.data\n",
    "    target = digits.target\n",
    "    scaler = StandardScaler()\n",
    "    data = scaler.fit_transform(data)\n",
    "    train_x = data[:1500].T; train_y = target[:1500]\n",
    "    test_x = data[1500:].T; test_y = target[1500:]\n",
    "    datalayer1 = Data(train_x, train_y, 256)\n",
    "    datalayer2 = Data(test_x, test_y, 297)\n",
    "    inner_layers = []\n",
    "    inner_layers.append(FullyConnect(8*8, 30))\n",
    "    inner_layers.append(Relu1())\n",
    "    inner_layers.append(FullyConnect(30, 10))\n",
    "    inner_layers.append(Sigmoid())\n",
    "    losslayer = QuadraticLoss(0)\n",
    "    accuracy = Accuracy()\n",
    "\n",
    "    for layer in inner_layers:\n",
    "        layer.lr = learning_rate # 为所有中间层设置学习速率\n",
    "\n",
    "    epochs = 1\n",
    "    for i in range(epochs):\n",
    "        print('epochs:', i)\n",
    "        losssum = 0\n",
    "        iters = 0\n",
    "        while True:\n",
    "            data, pos = datalayer1.forward()  # 从数据层取出数据\n",
    "            x, label = data\n",
    "            for layer in inner_layers:  # 前向计算\n",
    "                print(x[0][0])\n",
    "                x = layer.forward(x)\n",
    "\n",
    "            loss = losslayer.forward(x, label)  # 调用损失层forward函数计算损失函数值\n",
    "            losssum += loss\n",
    "            iters += 1\n",
    "            d = losslayer.backward()  # 调用损失层backward函数层计算将要反向传播的梯度\n",
    "            for layer in inner_layers[::-1]:  # 反向传播\n",
    "                d = layer.backward(d)\n",
    "\n",
    "            if pos == 0:  # 一个epoch完成后进行准确率测试\n",
    "                data, _ = datalayer2.forward()\n",
    "                x, label = data\n",
    "                for layer in inner_layers:\n",
    "                    x = layer.forward(x)\n",
    "                accu = accuracy.forward(x, label)  # 调用准确率层forward()函数求出准确率\n",
    "                print('loss:', losssum / iters)\n",
    "                print('accuracy:', accu)\n",
    "                break\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 417,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epochs: 0\n",
      "0.0\n",
      "-2.0469407920045453\n",
      "-0.20469407920045454\n",
      "-0.8974860182086454\n",
      "0.0\n",
      "-2.0964981090263777\n",
      "-0.2096498109026378\n",
      "-0.9806800498560426\n",
      "0.0\n",
      "-2.010054077890953\n",
      "-0.20100540778909531\n",
      "-1.0580078022922836\n",
      "0.0\n",
      "-2.004471838760559\n",
      "-0.2004471838760559\n",
      "-1.125633902953869\n",
      "0.0\n",
      "-1.9489294836612616\n",
      "-0.19489294836612617\n",
      "-1.1923505064991018\n",
      "0.0\n",
      "-1.9866772354258275\n",
      "-0.19866772354258277\n",
      "-1.242948108996307\n",
      "loss: 1.0338608922137364\n",
      "accuracy: 0.10101010101010101\n"
     ]
    }
   ],
   "source": [
    "x = multiplilyNet(0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler \n",
    "def main(mode, learning_rate = 1, L2 = 0):\n",
    "    digits = load_digits()\n",
    "    data = digits.data\n",
    "    target = digits.target\n",
    "    scaler = StandardScaler()\n",
    "    data = scaler.fit_transform(data)\n",
    "    \n",
    "    train_x = data[:1500].T; train_y = target[:1500]\n",
    "    test_x = data[1500:].T; test_y = target[1500:]\n",
    "    datalayer1 = Data(train_x, train_y, 256)\n",
    "    datalayer2 = Data(test_x, test_y, 297)\n",
    "    inner_layers = []\n",
    "    inner_layers.append(FullyConnect(8*8, 10, L2))\n",
    "    if mode == 'sigmoid':\n",
    "        #10\n",
    "        inner_layers.append(Sigmoid())\n",
    "    elif mode == 'relu':\n",
    "        #0.3\n",
    "        inner_layers.append(Relu())\n",
    "    elif mode == 'relu1':\n",
    "        #0.3\n",
    "        inner_layers.append(Relu1())\n",
    "    elif mode == 'tanh':\n",
    "        #0.2\n",
    "        inner_layers.append(Tanh())\n",
    "    losslayer = QuadraticLoss(L2)\n",
    "    accuracy = Accuracy()\n",
    "\n",
    "    for layer in inner_layers:\n",
    "        layer.lr = learning_rate # 为所有中间层设置学习速率\n",
    "\n",
    "    epochs = 20\n",
    "    for i in range(epochs):\n",
    "        print('epochs:', i)\n",
    "        losssum = 0\n",
    "        iters = 0\n",
    "        while True:\n",
    "            data, pos = datalayer1.forward()  # 从数据层取出数据\n",
    "            x, label = data\n",
    "            \n",
    "            for layer in inner_layers:  # 前向计算\n",
    "                x = layer.forward(x)\n",
    "\n",
    "            loss = losslayer.forward(x, label)  # 调用损失层forward函数计算损失函数值\n",
    "            losssum += loss\n",
    "            iters += 1\n",
    "            d = losslayer.backward()  # 调用损失层backward函数层计算将要反向传播的梯度\n",
    "            for layer in inner_layers[::-1]:  # 反向传播\n",
    "                d = layer.backward(d)\n",
    "\n",
    "            if pos == 0:  # 一个epoch完成后进行准确率测试\n",
    "                data, _ = datalayer2.forward()\n",
    "                x, label = data\n",
    "                for layer in inner_layers:\n",
    "                    x = layer.forward(x)\n",
    "                accu = accuracy.forward(x, label)  # 调用准确率层forward()函数求出准确率\n",
    "                print('loss:', losssum / iters)\n",
    "                print('accuracy:', accu)\n",
    "                break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 369,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epochs: 0\n",
      "loss: 1.2097735493412771\n",
      "accuracy: 0.5151515151515151\n",
      "epochs: 1\n",
      "loss: 0.3624268581499395\n",
      "accuracy: 0.6363636363636364\n",
      "epochs: 2\n",
      "loss: 0.3051000313557834\n",
      "accuracy: 0.7138047138047138\n",
      "epochs: 3\n",
      "loss: 0.27897049582571376\n",
      "accuracy: 0.7171717171717171\n",
      "epochs: 4\n",
      "loss: 0.27214479054078894\n",
      "accuracy: 0.7171717171717171\n",
      "epochs: 5\n",
      "loss: 0.268760480893708\n",
      "accuracy: 0.7205387205387206\n",
      "epochs: 6\n",
      "loss: 0.26565380519581977\n",
      "accuracy: 0.7171717171717171\n",
      "epochs: 7\n",
      "loss: 0.2634755952153021\n",
      "accuracy: 0.7171717171717171\n",
      "epochs: 8\n",
      "loss: 0.2608376021852095\n",
      "accuracy: 0.7138047138047138\n",
      "epochs: 9\n",
      "loss: 0.25869211071616305\n",
      "accuracy: 0.7138047138047138\n",
      "epochs: 10\n",
      "loss: 0.25627129760425793\n",
      "accuracy: 0.7138047138047138\n",
      "epochs: 11\n",
      "loss: 0.25414656387847284\n",
      "accuracy: 0.7138047138047138\n",
      "epochs: 12\n",
      "loss: 0.25187475524244224\n",
      "accuracy: 0.7138047138047138\n",
      "epochs: 13\n",
      "loss: 0.24978025329569356\n",
      "accuracy: 0.7138047138047138\n",
      "epochs: 14\n",
      "loss: 0.24765400292975828\n",
      "accuracy: 0.7138047138047138\n",
      "epochs: 15\n",
      "loss: 0.24564649936314628\n",
      "accuracy: 0.7138047138047138\n",
      "epochs: 16\n",
      "loss: 0.24366122040810037\n",
      "accuracy: 0.7138047138047138\n",
      "epochs: 17\n",
      "loss: 0.24175352298601804\n",
      "accuracy: 0.7138047138047138\n",
      "epochs: 18\n",
      "loss: 0.23990347338837217\n",
      "accuracy: 0.7138047138047138\n",
      "epochs: 19\n",
      "loss: 0.2381012447707017\n",
      "accuracy: 0.7138047138047138\n"
     ]
    }
   ],
   "source": [
    "main('relu1', 0.3, L2 = 200)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 614,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1.42857143 0.         4.28571429]\n",
      " [2.85714286 0.         5.71428571]]\n"
     ]
    }
   ],
   "source": [
    "weights = np.array([\n",
    "    [1,2,3],\n",
    "    [2,3,4]\n",
    "])\n",
    "iskeep = np.array([\n",
    "    [1,0,1]\n",
    "])\n",
    "print(weights*iskeep/0.7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 658,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.38383888, 0.45623651, 0.36905302, 0.121025  , 0.41894779]])"
      ]
     },
     "execution_count": 658,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.random.rand(1,5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "10-8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1e-07"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "10e-8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1e-08"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "1e-8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "8"
      ]
     },
     "execution_count": 119,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.power(2,3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'np' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-1-832c7515f1cb>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mz\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'np' is not defined"
     ]
    }
   ],
   "source": [
    "np.z"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0 1 4]\n",
      " [1 2 2]]\n"
     ]
    }
   ],
   "source": [
    "a1 = np.array([\n",
    "    [1,2,5],\n",
    "    [3,4,4]\n",
    "])\n",
    "a2 = np.array([\n",
    "    [1],\n",
    "    [2]\n",
    "])\n",
    "print(a1-a2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[2.66666667]\n",
      " [3.66666667]]\n"
     ]
    }
   ],
   "source": [
    "print(np.mean(a1, axis = 1, keepdims=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 190,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1, 2, 5],\n",
       "       [6, 8, 8]])"
      ]
     },
     "execution_count": 190,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a2*a1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
